{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hw2skeleton import io, cluster, utils\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in 136 active sites\n"
     ]
    }
   ],
   "source": [
    "#path = '/Users/Sisi/Documents/GitHub/BMI-203-HW2/data' #Note: may have to change the path when uploading into GitHub?\n",
    "path = '/Users/sierra.lear/Documents/GitHub/BMI-203-HW2/data'\n",
    "\n",
    "active_site = io.read_active_sites(path);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_site[0].residues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_site[3].residues[0].type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_res = []\n",
    "for r in active_site[0].residues:\n",
    "    list_res.append(r.type)\n",
    "list_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_res_2 = [r.type for r in active_site[0].residues]\n",
    "list_res_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective #1:\n",
    "*Implement a similarity metric to compare active sites.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my similarity metric, I want to calculate the number of a given amino acid in an active site. I can then calculate the Euclidan distance based on the 20-D space constructed by the ratio of all 20 different amino acids.\n",
    "\n",
    "One one hand, this obviously simplifies or \"ignores\" important functional information, namely the 3D structure/configuration of amino acids or residues in relation to each other as well as the number of residues or how \"large\" an active site is.\n",
    "\n",
    "However, given my inexperience with enzyme function, I am hypothesizing that the overall \"chemical identity\" of a residue still gives an important clue to its function--in particular, two active sites that contain more of the same amino acids are more likely to be functionally similar or have similar roles than two residues that contain no overlapping amino acids. Furthermore, given that some amino acids have specific charges or hydrophobicity, I know that amino acids often drive structure, so I also hypothesize that some of this \"lost\" structural information will actually be redundant or contained within the ratio or identity of amino acids anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(active_site)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combos = list(product(range(0,len(active_site)), range(0,len(active_site))))\n",
    "combos_test =list(product(range(0,len(active_site)), range(0,len(active_site))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_aa = ['ALA', 'ARG', 'ASN', 'ASP', 'CYS', 'GLN', 'GLU', 'GLY', 'HIS', 'ILE', 'LEU', 'LYS',\n",
    "         'MET', 'PHE', 'PRO', 'SER', 'THR', 'TRP', 'TYR', 'VAL']\n",
    "\n",
    "def compute_similarity_partitioning(site_a, centroid):\n",
    "    \"\"\"\n",
    "    Compute the similarity between two given ActiveSite instances.\n",
    "\n",
    "    Input: one ActiveSite instance, one \"centroid\" consisting of a 20-element vector with numnbers between 0 and 1\n",
    "    Output: the similarity between them (a floating point number)\n",
    "    \"\"\"\n",
    "\n",
    "    similarity = 0.0\n",
    "    \n",
    "    #creating list comprehension of all AAs for site a residues\n",
    "    list_res_a = [r.type for r in site_a.residues]\n",
    "    \n",
    "    #create two histogram count lists for site a and b from the list comprehensions\n",
    "    count_a = Counter(list_res_a)\n",
    "    \n",
    "    #initialize two 20-element AA dictionaries: one for site A, one for site B\n",
    "    a_dict = {aa:0 for aa in my_aa}\n",
    "    \n",
    "    #convert normalized histogram count into the 20-D vector AA dictionary\n",
    "    #a site\n",
    "    for aa, count in count_a.items():\n",
    "        a_dict[aa] = count\n",
    "    a_vector = np.array(list(a_dict.values()))\n",
    "    a_vector = a_vector/np.sum(a_vector) #to return percentage of amino acid identity\n",
    "        \n",
    "    #calculate Euclidian distance between the two sites' 20-D vectors\n",
    "    similarity = np.sqrt(np.sum((a_vector - centroid)**2))\n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_aa = ['ALA', 'ARG', 'ASN', 'ASP', 'CYS', 'GLN', 'GLU', 'GLY', 'HIS', 'ILE', 'LEU', 'LYS',\n",
    "         'MET', 'PHE', 'PRO', 'SER', 'THR', 'TRP', 'TYR', 'VAL']\n",
    "\n",
    "def compute_similarity(site_a, site_b):\n",
    "    \"\"\"\n",
    "    Compute the similarity between two given ActiveSite instances.\n",
    "\n",
    "    Input: two ActiveSite instances\n",
    "    Output: the similarity between them (a floating point number)\n",
    "    \"\"\"\n",
    "\n",
    "    similarity = 0.0\n",
    "    \n",
    "    #creating list comprehension of all AAs for site a residues\n",
    "    list_res_a = [r.type for r in site_a.residues]\n",
    "    \n",
    "    #creating list comprehension of all AAs for site b residues\n",
    "    list_res_b = [r.type for r in site_b.residues]\n",
    "    \n",
    "    #create two histogram count lists for site a and b from the list comprehensions\n",
    "    count_a = Counter(list_res_a)\n",
    "    count_b = Counter(list_res_b)\n",
    "    \n",
    "    #initialize two 20-element AA dictionaries: one for site A, one for site B\n",
    "    a_dict = {aa:0 for aa in my_aa}\n",
    "    b_dict = {aa:0 for aa in my_aa}\n",
    "    \n",
    "    #convert normalized histogram count into the 20-D vector AA dictionary\n",
    "    #a site\n",
    "    for aa, count in count_a.items():\n",
    "        a_dict[aa] = count\n",
    "    a_vector = np.array(list(a_dict.values()))\n",
    "    a_vector = a_vector/np.sum(a_vector) #to return percentage of amino acid identity\n",
    "    #b site\n",
    "    for aa, count in count_b.items():\n",
    "        b_dict[aa] = count\n",
    "    b_vector = np.array(list(b_dict.values()))\n",
    "    b_vector = b_vector/np.sum(b_vector)\n",
    "        \n",
    "    #calculate Euclidian distance between the two sites' 20-D vectors\n",
    "    similarity = np.sqrt(np.sum((a_vector - b_vector)**2))\n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_similarity(active_site[3], active_site[4]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = ['Site_A', 'Site_B', 'Similarity'])\n",
    "for a, b in combos:\n",
    "    similarity = compute_similarity(active_site[a], active_site[b])\n",
    "    df = df.append({'Site_A': a, 'Site_B': b, 'Similarity': similarity}, ignore_index=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[~(df[\"Site_A\"] == df[\"Site_B\"]), :] #remove all rows where Site_A == Site_B\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df[\"Similarity\"], bins=50, norm_hist=False, kde=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax_me = plt.subplots()\n",
    "sns.distplot(df[\"Similarity\"], bins=50, norm_hist=False, kde=False, ax=ax_me)\n",
    "ax_me.set(ylim=[0,300])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective #2: \n",
    "*Implement a partitioning algorithm to cluster the set of active sites.* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = pd.DataFrame(index = '', columns = ['Site_A', 'Site_B', 'Similarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_site[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.MultiIndex.from_product([['a.name'], [0], np.arange(0,20)], \n",
    "                                      names=['original_id', 'sequential_id', 'amino_acid_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pd.DataFrame()\n",
    "\n",
    "for i, a in enumerate(active_site):\n",
    "    list_res = [r.type for r in a.residues]\n",
    "    count_aa = Counter(list_res)\n",
    "    aa_dict = {aa:0 for aa in my_aa}\n",
    "    for aa, count in count_aa.items():\n",
    "        aa_dict[aa] = count\n",
    "    aa_vec = np.array(list(aa_dict.values()))\n",
    "    midx = pd.MultiIndex.from_product([[i], np.arange(0,len(aa_vec))], \n",
    "                                      names=['sequential_id', 'amino_acid_ID'])\n",
    "    temp_df = pd.DataFrame({'aa_counts' : aa_vec}, index=midx)\n",
    "    output_df = output_df.append(temp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_slicer = pd.IndexSlice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.loc[idx_slicer[0,:],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.unstack('sequential_id').T.apply(lambda x: x - x.mean(), axis = 1).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.drop().unstack('original_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "    #creating list comprehension of all AAs for site a residues\n",
    "    list_res_a = [r.type for r in site_a.residues]\n",
    "    \n",
    "    #creating list comprehension of all AAs for site b residues\n",
    "    list_res_b = [r.type for r in site_b.residues]\n",
    "    \n",
    "    #create two histogram count lists for site a and b from the list comprehensions\n",
    "    count_a = Counter(list_res_a)\n",
    "    count_b = Counter(list_res_b)\n",
    "    \n",
    "    #initialize two 20-element AA dictionaries: one for site A, one for site B\n",
    "    a_dict = {aa:0 for aa in my_aa}\n",
    "    b_dict = {aa:0 for aa in my_aa}\n",
    "    \n",
    "    #convert histogram count into the 20-D vector AA dictionary\n",
    "    #a site\n",
    "    for aa, count in count_a.items():\n",
    "        a_dict[aa] = count\n",
    "    a_vector = np.array(list(a_dict.values()))\n",
    "    #b site\n",
    "    for aa, count in count_b.items():\n",
    "        b_dict[aa] = count\n",
    "    b_vector = np.array(list(b_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_site[3].residues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_by_partitioning(active_sites):\n",
    "    \"\"\"\n",
    "    Cluster a given set of ActiveSite instances using a partitioning method.\n",
    "    Input: a list of ActiveSite instances\n",
    "    Output: a clustering of ActiveSite instances\n",
    "            (this is really a list of clusters, each of which is list of\n",
    "            ActiveSite instances)\n",
    "    \"\"\"\n",
    "\n",
    "    #number of clusters\n",
    "    k = 5 #based off histogram--while an elbow plot would be a better way to pick clusters, I chose\n",
    "              #this for the sake of brevity\n",
    "    \n",
    "    #creating random centroids\n",
    "        # my centroid will be a 20-element vector with randomized numbers between 0 and 1\n",
    "    np.random.seed(5)\n",
    "    centroid_list = [None] * k #initializing my list of k centroids\n",
    "    for i in range(0,len(centroid_list)): #for loop to create and store all my centroids in my centroid_list\n",
    "        centroid_list[i] = np.random.rand(20) #creating random numbers between 0 and 1 to fill in the 20 elements of each centroid vector\n",
    "    \n",
    "    \n",
    "    #k-means algorithm    \n",
    "    partition_combos = list(product(range(0,len(active_site)), range(0,len(centroid_list)))) #create list of tuples\n",
    "                                                                                          #with all combinations of\n",
    "                                                                                #active site instances and centroids\n",
    "        \n",
    "    df = pd.DataFrame(columns = ['ActiveSite', 'Centroids', 'Similarity'])\n",
    "    for a, b in partition_combos:\n",
    "        similarity = compute_similarity_partitioning(active_site[a], centroid_list[b])\n",
    "        df = df.append({'ActiveSite': a, 'Centroids': b, 'Similarity': similarity}, ignore_index=True)\n",
    "        df = df.reset_index()\n",
    "    \n",
    "    #labeling each ActiveSite to a centroid based on its similarity score\n",
    "        #groupby ActiveSite and take minimum distance --> delete all the rows that don't have the min distance,\n",
    "        #thus I'm only keeping the cells with the \"labeled centroid\" for each ActiveSite instance\n",
    "        sorted_df = df.sort_values(\"Similarity\").groupby([\"ActiveSite\"]).first()\n",
    "        \n",
    "    \n",
    "    #Move centroid to the center of the cluster based on the mean\n",
    "        #groupby centroid\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #     #create df containing \"20-element vectors\" based on number of each amino acid in a given ActiveSite instance\n",
    "#     aav_df = pd.DataFrame() #initialize df\n",
    "\n",
    "#     for i, a in enumerate(active_site): #looping through all 136 active sites\n",
    "#         list_res = [r.type for r in a.residues] #creating list comprehension of all AAs for an active site\n",
    "#         count_aa = Counter(list_res) #create histogram count list for active site from the list comprehensions\n",
    "#         aa_dict = {aa:0 for aa in my_aa} #initialize 20-element AA dictionary\n",
    "#         for aa, count in count_aa.items(): #convert histogram count into the 20-D vector AA dictionary\n",
    "#             aa_dict[aa] = count\n",
    "#         aa_vec = np.array(list(aa_dict.values()))\n",
    "#         midx = pd.MultiIndex.from_product([[i], np.arange(0,len(aa_vec))], #create multiindex for final df\n",
    "#                                       names=['sequential_id', 'amino_acid_ID'])\n",
    "#         temp_df = pd.DataFrame({'aa_counts' : aa_vec}, index=midx)\n",
    "#         aav_df = aav_df.append(temp_df)\n",
    "#     aav_df = aav_df.unstack('sequential_id').T #pivot to create final matrix, tranposed so fits orientation of PCA in scikitlearn\n",
    "#     aav_df = aav_df.apply(lambda x: ((x - x.mean())/np.var(x,ddof=1)), axis = 1)\n",
    "    \n",
    "    #perform PCA to further reduce points from 20-D space to 2-D space\n",
    " #   pca = PCA(n_components=2)\n",
    " #   proj_aa = pca.fit_transform(aav_df.to_numpy())\n",
    "    \n",
    "    #number of clusters\n",
    " #   k = 4 #picking 4 because histogram of similarity scores above shows 4 \"humps\"\n",
    "    \n",
    "    #picking random coordinates\n",
    "    #coord_x = np.random.rand(0, np.max(), size = k) #random x coordinate\n",
    "    \n",
    "\n",
    "    return sorted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot insert level_0, already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-135-89d92b52427e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcluster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcluster_by_partitioning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactive_site\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-134-6717df4bb6f8>\u001b[0m in \u001b[0;36mcluster_by_partitioning\u001b[0;34m(active_sites)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0msimilarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_similarity_partitioning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactive_site\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcentroid_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'ActiveSite'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Centroids'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Similarity'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msimilarity\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m#labeling each ActiveSite to a centroid based on its similarity score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mreset_index\u001b[0;34m(self, level, drop, inplace, col_level, col_fill)\u001b[0m\n\u001b[1;32m   4692\u001b[0m                 \u001b[0;31m# to ndarray and maybe infer different dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4693\u001b[0m                 \u001b[0mlevel_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_maybe_casted_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4694\u001b[0;31m                 \u001b[0mnew_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4696\u001b[0m         \u001b[0mnew_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36minsert\u001b[0;34m(self, loc, column, value, allow_duplicates)\u001b[0m\n\u001b[1;32m   3574\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3575\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbroadcast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3576\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_duplicates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_duplicates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3578\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36minsert\u001b[0;34m(self, loc, item, value, allow_duplicates)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_duplicates\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m             \u001b[0;31m# Should this be a different kind of error??\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1173\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cannot insert {}, already exists\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot insert level_0, already exists"
     ]
    }
   ],
   "source": [
    "cluster = cluster_by_partitioning(active_site)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Centroids</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ActiveSite</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.167310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.171325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.181629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.211896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.201248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.171325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.228054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.171325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.186532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.188231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Centroids  Similarity\n",
       "ActiveSite                       \n",
       "0.0               0.0    2.167310\n",
       "1.0               0.0    2.171325\n",
       "2.0               0.0    2.181629\n",
       "3.0               0.0    2.211896\n",
       "4.0               0.0    2.201248\n",
       "...               ...         ...\n",
       "131.0             0.0    2.171325\n",
       "132.0             0.0    2.228054\n",
       "133.0             0.0    2.171325\n",
       "134.0             1.0    2.186532\n",
       "135.0             0.0    2.188231\n",
       "\n",
       "[136 rows x 2 columns]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Similarity'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(cluster[\"Similarity\"], bins=50, norm_hist=False, kde=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(cluster[:,0], cluster[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of clusters\n",
    "k = 3\n",
    "# X coordinates of random centroids\n",
    "C_x = np.random.randint(0, np.max(X)-20, size=k)\n",
    "# Y coordinates of random centroids\n",
    "C_y = np.random.randint(0, np.max(X)-20, size=k)\n",
    "C = np.array(list(zip(C_x, C_y)), dtype=np.float32)\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(list(save_this.values()))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_v = np.array(['ALA', 'ALA', 'GLY', 'PRO'])\n",
    "test_hist, b = np.histogram(test_v, ['ALA', 'GLY', 'PRO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = {'san francisco' : 'david',4 : 'sierra'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict['kevin'] = 'ft collin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_aa = ['ALA', 'ARG', 'ASN', 'ASP', 'CYS', 'GLN', 'GLU', 'GLY', 'HIS', 'ILE', 'LEU', 'LYS',\n",
    "         'MET', 'PHE', 'PRO', 'SER', 'THR', 'TRP', 'TYR', 'VAL']\n",
    "my_aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_aa_dict = {}\n",
    "for aa in my_aa:\n",
    "    sample_aa_dict[aa] = 0\n",
    "print(sample_aa_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "different_aa_dict = {aa:0 for aa in my_aa}\n",
    "different_aa_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "test_v = np.array(['ALA', 'ALA', 'GLY', 'PRO'])\n",
    "count_v = Counter(test_v)\n",
    "sns.barplot(list(count_v.keys()), list(count_v.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for aa, count in count_v.items():\n",
    "    sample_aa_dict[aa] = count\n",
    "sample_aa_dict\n",
    "sns.barplot(list(sample_aa_dict.keys()), list(sample_aa_dict.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective #3: \n",
    "*Implement an agglomerative algorithm to cluster the set of active sites.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_hierarchically(active_sites):\n",
    "    \"\"\"\n",
    "    Cluster the given set of ActiveSite instances using a hierarchical algorithm.                                                                  #\n",
    "\n",
    "    Input: a list of ActiveSite instances\n",
    "    Output: a list of clusterings\n",
    "            (each clustering is a list of lists of Sequence objects)\n",
    "    \"\"\"\n",
    "\n",
    "    # Create tuples listing every combination of active site with every other active site\n",
    "    combos = list(product(range(0,len(active_site)), range(0,len(active_site))))\n",
    "    \n",
    "    #create data frame consisting of two active sites and the distance between them using my similarity metric\n",
    "    df = pd.DataFrame(columns = ['Site_A', 'Site_B', 'Similarity'])\n",
    "    for a, b in combos:\n",
    "        similarity = compute_similarity(active_site[a], active_site[b])\n",
    "        df = df.append({'Site_A': a, 'Site_B': b, 'Similarity': similarity}, ignore_index=True)\n",
    "    df = df.loc[~(df[\"Site_A\"] == df[\"Site_B\"]), :] #remove all comparisons of an active site with itself\n",
    "    \n",
    "    #sort list of distances from lowest to highest distance (labelled \"similarity\") in df\n",
    "    df = df.sort_values(by = [\"Similarity\"])\n",
    "\n",
    "    return df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_hierarchically(active_sites):\n",
    "    \"\"\"\n",
    "    Cluster the given set of ActiveSite instances using a hierarchical algorithm.                                                                  #\n",
    "\n",
    "    Input: a list of ActiveSite instances\n",
    "    Output: a list of clusterings\n",
    "            (each clustering is a list of lists of Sequence objects)\n",
    "    \"\"\"\n",
    "\n",
    "    # Create tuples listing every combination of active site with every other active site\n",
    "    combos = list(product(range(0,len(active_site_instance)), range(0,len(active_site_instance))))\n",
    "    \n",
    "    #create data frame consisting of two active sites and the distance between them using my similarity metric\n",
    "    similarity_df = pd.DataFrame(columns = ['Site_A', 'Site_B', 'Similarity', \"Cluster\"])\n",
    "    for i, tup in enumerate(combos):\n",
    "        similarity = compute_similarity_test(active_site_instance.loc[combos[tup][0]], active_site_instance.loc[[tup][1]])\n",
    "        similarity_df = similarity_df.append({'Site_A': a, 'Site_B': b, 'Similarity': similarity, \"Cluster\": i}, ignore_index=True)\n",
    "        similarity_df = similarity_df.loc[~(similarity_df[\"Site_A\"] == similarity_df[\"Site_B\"]), :] #remove all rows where Site_A == Site_B  \n",
    "    \n",
    "    #sort list of distances from lowest to highest distance (labelled \"similarity\") in df\n",
    "    similarity_df = similarity_df.sort_values(by = [\"Similarity\"])\n",
    "    \n",
    "#     cluster_0 = [] #initialize cluster list\n",
    "#     #add two closest active site clusters to cluster list\n",
    "#     merged.append(similarity_df.iloc[0, 0])\n",
    "#     merged.append(similarity_df.iloc[0, 1])\n",
    "    \n",
    " \n",
    "        \n",
    "\n",
    "    \n",
    "    #need to delete duplicates in each cluster list at the end\n",
    "    \n",
    "\n",
    "\n",
    "    return similarity_df, merged\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-2cec40fec22f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcluster_hierarchically\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactive_site\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-35-5b43a93b8737>\u001b[0m in \u001b[0;36mcluster_hierarchically\u001b[0;34m(active_sites)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0msimilarity_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Site_A'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Site_B'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Similarity'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cluster\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0msimilarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_similarity_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactive_site_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcombos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactive_site_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0msimilarity_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimilarity_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Site_A'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Site_B'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Similarity'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msimilarity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cluster\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0msimilarity_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimilarity_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilarity_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Site_A\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0msimilarity_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Site_B\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#remove all rows where Site_A == Site_B\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "cluster, merge = cluster_hierarchically(active_site)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Site_A</th>\n",
       "      <th>Site_B</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>6085</td>\n",
       "      <td>45.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14132</td>\n",
       "      <td>104.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12693</td>\n",
       "      <td>94.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18164</td>\n",
       "      <td>134.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4955</td>\n",
       "      <td>36.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Site_A  Site_B  Similarity\n",
       "6085     45.0    10.0         0.0\n",
       "14132   104.0    92.0         0.0\n",
       "12693    94.0     3.0         0.0\n",
       "18164   134.0    74.0         0.0\n",
       "4955     36.0    96.0         0.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[45.0, 10.0]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "can't assign to function call (<ipython-input-30-65bfeda5e6be>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-30-65bfeda5e6be>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    merged_value(counter) = 5\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m can't assign to function call\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "merged_value(counter) = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
